// Each #kernel tells which function to compile; you can have many kernels
#pragma kernel CSComputeAccel

// Number of threads per thread group. Should match with the CPU code invoking this!
#define ThreadGroupSize 64

#define PI 3.14159265
// The kernel's finite support. Note that the support radius is 2*KernelSupport (goes exactly to 0 at +- radius).
//TODO: use constant buffers for constants, and pass them using structs
#define KernelSupport 1
#define sInv (1 / KernelSupport)

// TODO: No reason for this number, change it later
#define ViscosityCoef 1

//	https://docs.unity3d.com/Manual/SL-PlatformDifferences.html "Using buffers with GPU buffers"
//cbuffer _ParticleConstantsBuf : register(b0)

// For debugging. Create a RenderTexture with enableRandomWrite flag and set it
// with cs.SetTexture
//RWTexture2D<float4> Result;
RWTexture3D<float> Result;

uint _MaxNumParticles;
half _ParticleMass;
half _PressureStiffness;
half _ReferenceDensity;
half _Gravity;				// Magnitude of force due to gravity

//TODO: why register? 
// Position inputs
StructuredBuffer<float3> _ParticlePositionInputBuf;//: register(t0);

// Acceleration outputs
RWStructuredBuffer<float3> _ParticleAccelOutputBuf;//: register(u0);


//TODO: is there a max of about 16000 particles? Because of groupshared memory limit and float3 size

// The kernel function varies with distance from the center. If xi is the position of particle i and 
//  x is some point in space, r=x-xi is the distance between them.
//TODO: optimizations: branchless, precompute formula, MAD
float Kernel(float r)
{
    // Textbook pg 289
	float rOverS = r * sInv;
	
	// 1 / (pi*s^3)
	//float piFactor = pow(sInv, 3) / 3.1415;
	float piFactor = 1 / (PI * pow(KernelSupport, 3));
    
	if (rOverS < 0 || rOverS > 2)
		return 0;
	else if (rOverS <= 1)
        // Between 0 <= r/s <= 1
		return piFactor * (1 - 1.5 * pow(rOverS, 2) + 0.75 * pow(rOverS, 3));
	else if (rOverS <= 2)
		// Between 1 <= r/s <= 2
		return piFactor * (0.25 * pow(2 - rOverS, 3));
    
}

float KernelGradient(float r)
{
	float rOverS = r * sInv;
	float piFactor = 1 / (PI * pow(KernelSupport, 4));

	if (rOverS < 0 || rOverS > 2)
		return 0;
	else if (rOverS <= 1)
        // Between 0 <= r/s <= 1
		return piFactor * 3 * rOverS * mad(0.75, rOverS, -1);
	else if (rOverS <= 2)
		// Between 1 <= r/s <= 2
		return piFactor * (-0.75 * pow(2 - rOverS, 2));
}
float KernelLaplacian(float r)
{
	float rOverS = r * sInv;
	float piFactor = 1 / (PI * pow(KernelSupport, 5));

	if (rOverS < 0 || rOverS > 2)
		return 0;
	else if (rOverS <= 1)
        // Between 0 <= r/s <= 1
		return piFactor * 3 * mad(1.5, rOverS, -1);
	else if (rOverS <= 2)
		// Between 1 <= r/s <= 2
		return piFactor * mad(1.5, -rOverS, 3);		// (3/2)*(2 - rOverS) = 3 - 1.5*rOverS
}

float Pressure(float density)
{
	return _PressureStiffness * (
}

uint2 flatTo2D(uint1 val, int w)
{
	return uint2(fmod(val, w), floor(val / w));
}

uint3 flatTo3D(uint1 val, int w, int h)
{
	return uint3(fmod(val, w), fmod(val / w, h), floor(val / (w * h)));
}

void TestOutput(uint3 id)
{
	
    // TODO: insert actual code here!
    //Result[id.xy] = float4(id.x & id.y, (id.x & 15)/15.0, (id.y & 15)/15.0, 0.0);

	// Gives a value between 0 and 2.5, normalize to 0 to 1
	//float k = Kernel(distance(id.xy, uint2(128, 128)));
	//Result[id.xy] = float4(k, k, k, 1);
	
	//Result[id.xy] = float4(k, k, k, 1);
	
	float w = _MaxNumParticles;
	//uint2 id2D = uint2(fmod(id.x, w), floor(id.x / w));
	//Result[id2D.xy] = float4(id2D.x & id2D.y, (id2D.x & 15) / 15.0, (id2D.y & 15) / 15.0, 0.0) * _ParticleMass;
	
	uint3 id3D = flatTo3D(id.x, w, w);
	//Result[id3D] = float4(1.0f, 0.0f, 0.0f, 1.0f);
	Result[id3D.xyz] = float3(id3D.x & id3D.y & id3D.z, (id3D.x & 15) / 15.0, (id3D.y & 15) / 15.0) * _ParticleMass;
}


// Density and pressure data for interim calculations. Also do kernels?
//	groupshared is for sharing data among threads in a single thread group
groupshared float3 particleDensity[ThreadGroupSize];
groupshared float3 particlePressure[ThreadGroupSize];
groupshared float particleKernel[ThreadGroupSize];		// Reused for the gradient and laplacian also

// numthreads(x,y,z) declares that each thread group is a grid of x*y*z threads. A multiple of 64 threads is a good number bc the thread warp size is either 32 or 64 depending on gpu.
[numthreads(ThreadGroupSize, 1, 1)]

// The group index is the flattened index of a thread within its group.
void CSComputeAccel(uint3 id : SV_DispatchThreadID, uint groupIdx : SV_GroupIndex)
{
	// Position of this particle
	float3 particlePos = _ParticlePositionInputBuf[groupIdx];

	//TestOutput(id);

	//uint3 id3D = flatTo3D(id.x, _MaxNumParticles, _MaxNumParticles);
	//float2 tmp = float4(particlePos, 0);
	//Result[id3D] = tmp;

	// loop through all (TODO: nearby) particles, summing their total contribution from the kernel function
	for (int i = 0; i < _MaxNumParticles; i++)
	{
		float dist = distance(particlePos, _ParticlePositionInputBuf[i]);
		float kernelVal = Kernel(dist);

		particleKernel[id.x] = kernelVal;
		particleDensity[id.x] = _ParticleMass * kernelVal;
	}
	
	GroupMemoryBarrierWithGroupSync();
	
	float thisDensity = particleDensity[id.x];

	// calculate pressure and pressure gradient
	for (int i = 0; i < _MaxNumParticles; i++)
	{
		float thisPressure = _PressureStiffness * (particleDensity[i] - thisDensity);

	}
	//GroupMemoryBarrierWithGroupSync();
	
	// calculate vector field laplacian
	
	//GroupMemoryBarrierWithGroupSync();
	
	}
